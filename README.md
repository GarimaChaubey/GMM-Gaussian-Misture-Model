# Gaussian Mixture Models (GMM) ‚Äî Full Guide with Intuition, Math, and Code

This repository provides a complete, beginner-friendly yet mathematically clear explanation of **Gaussian Mixture Models (GMM)** along with **EM Algorithm**, **visualizations**, and **comparison with K-Means**.  
Perfect for students, beginners in Machine Learning, and anyone trying to understand clustering beyond K-Means.

---

# üìò Table of Contents

1. [Introduction](#introduction)  
2. [Why GMM?](#why-gmm)  
3. [Understanding a Single Gaussian](#understanding-a-single-gaussian)  
4. [Gaussian Mixture Model](#gaussian-mixture-model)  
5. [Latent Variables & Generative View](#latent-variables--generative-view)  
6. [EM Algorithm ‚Äî Complete Explanation](#em-algorithm--complete-explanation)  
7. [GMM Pseudocode](#gmm-pseudocode)  
8. [GMM vs K-Means](#gmm-vs-k-means)  
9. [Python Code for GMM](#python-code-for-gmm)  
10. [Python Code for K-Means Comparison](#python-code-for-k-means-comparison)  
11. [When Should You Use GMM?](#when-should-you-use-gmm)  
12. [Project Structure](#project-structure)  
13. [References](#references)

---

# üß† Introduction

A **Gaussian Mixture Model (GMM)** is a **probabilistic clustering model** that assumes the data is generated from a mixture of multiple **Gaussian (Normal) distributions**.

Unlike K-Means, GMM provides:

- **Soft clustering** (probabilistic cluster membership)  
- **Elliptical cluster shapes** (via covariance)  
- **Full density estimation**:  
  $
  p(x) = \sum_{k=1}^{K} \pi_k \, \mathcal{N}(x | \mu_k, \Sigma_k)
  $

GMM is trained using the **Expectation‚ÄìMaximization (EM)** algorithm.

---

# ‚ùì Why GMM?

K-Means assumes:

- Clusters are **circular**
- All clusters have **same size**
- Hard assignment (only 1 cluster)

Real-world data rarely behaves like this.

GMM solves these problems because:

‚úî It models **overlapping clusters**  
‚úî It supports **arbitrary covariance** ‚Üí elliptical shapes  
‚úî It gives **probabilities**, not just labels  

---

# üìê Understanding a Single Gaussian

A \(d\)-dimensional Gaussian distribution is:

\[
\mathcal{N}(x|\mu,\Sigma)=
\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}
\exp\left(
-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)
\right)
\]

- **Œº (mean):** center of distribution  
- **Œ£ (covariance):** shape, size, orientation  

---

# üß© Gaussian Mixture Model

A GMM assumes data is generated by **multiple Gaussians**:

\[
p(x)=\sum_{k=1}^{K}\pi_k\,\mathcal{N}(x|\mu_k,\Sigma_k)
\]

Where:

| Parameter | Meaning |
|----------|---------|
| \( \pi_k \) | Weight (how big each cluster is) |
| \( \mu_k \) | Cluster center |
| \( \Sigma_k \) | Cluster shape/orientation |

---

# üé≠ Latent Variables & Generative View

For each point:

1. A hidden variable \( z_i \) chooses which Gaussian to use:  
   \[
   P(z_i = k) = \pi_k
   \]
2. Then,  
   \[
   x_i \sim \mathcal{N}(\mu_k, \Sigma_k)
   \]

These labels \(z_i\) are **not observed** ‚Üí they are *latent*.

GMM + EM algorithm helps infer both:

- which cluster generated which point (soft probabilities)  
- parameters Œº, Œ£, œÄ  

---

# üîÅ EM Algorithm ‚Äî Complete Explanation

EM solves the problem:

> Maximize likelihood of data when cluster assignments are unknown.

Direct maximization is hard due to latent variables, so EM alternates between two steps:

---

## **üü¶ E-Step: Compute Responsibilities**

Compute probability that point \(x_i\) belongs to cluster \(k\):

\[
\gamma_{ik}
= \frac{\pi_k \, \mathcal{N}(x_i|\mu_k,\Sigma_k)}
       {\sum_{j=1}^{K}\pi_j \mathcal{N}(x_i|\mu_j,\Sigma_j)}
\]

Interpretation:

- Œ≥·µ¢‚Çñ = ‚Äúpoint i belongs to cluster k with probability Œ≥·µ¢‚Çñ‚Äù
- Soft cluster assignments

---

## **üüß M-Step: Update Parameters Using Responsibilities**

Let:

\[
N_k = \sum_{i=1}^{N} \gamma_{ik}
\]

Then:

### Update mixing weights:
\[
\pi_k = \frac{N_k}{N}
\]

### Update means:
\[
\mu_k = \frac{1}{N_k} \sum_{i=1}^N \gamma_{ik} x_i
\]

### Update covariance:
\[
\Sigma_k = \frac{1}{N_k}
\sum_{i=1}^N \gamma_{ik}(x_i-\mu_k)(x_i-\mu_k)^T
\]

---

## **üü© Log-Likelihood & Convergence**

After every iteration:

\[
\log L =
\sum_{i=1}^N \log 
\left(
\sum_{k=1}^{K} 
\pi_k \mathcal{N}(x_i|\mu_k,\Sigma_k)
\right)
\]

EM stops when:

\[
|\log L_{\text{new}} - \log L_{\text{old}}| < \epsilon
\]

---

# üßæ GMM Pseudocode

Initialize Œºk, Œ£k, œÄk

repeat:
    # E-step
    compute responsibilities Œ≥ik

    # M-step
    update Œºk using weighted mean
    update Œ£k using weighted covariance
    update œÄk using effective counts Nk

    compute log-likelihood

until convergence
